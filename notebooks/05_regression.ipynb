{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/skew/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pytz import timezone \n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from skew_correction.helper import *\n",
    "from skew_correction.data import plot_random_images\n",
    "from skew_correction.model import total_params, print_metrics_on_epoch_end, get_acc\n",
    "\n",
    "root_dir = \"/\".join( os.getcwd().split(\"/\")[:-1])\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "root_dir\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data (only run once, DO NOT RUN ANYMORE )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split files from one folder into 2 tran-test\n",
    "\n",
    "def split_files(src_dir, dest_dir, train_size):\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    \n",
    "    all_files = os.listdir(src_dir)\n",
    "    random.shuffle(all_files)\n",
    "    \n",
    "    total_files = len(all_files)\n",
    "    train_count = int(total_files * train_size)\n",
    "    \n",
    "    train_files = all_files[:train_count]\n",
    "    test_files = all_files[train_count:]\n",
    "    \n",
    "    train_dir = os.path.join(dest_dir, \"train_\")\n",
    "    test_dir = os.path.join(dest_dir, \"test_\")\n",
    "    \n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    \n",
    "    move_files(src_dir, train_dir, train_files)\n",
    "    move_files(src_dir, test_dir, test_files)\n",
    "    return train_files, test_files\n",
    "\n",
    "def move_files(src_dir, dest_dir, files):\n",
    "    for file in files:\n",
    "        src_path = os.path.join(src_dir, file)\n",
    "        dest_path = os.path.join(dest_dir, file)\n",
    "        shutil.move(src_path, dest_path)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "src_directory = os.path.join(data_dir, \"original/train\")\n",
    "dest_directory = os.path.join(data_dir, \"original/\")\n",
    "train_ratio = 0.8  # 80% for training, 20% for testing\n",
    "\n",
    "## train_files, test_files = split_files(src_directory, dest_directory, train_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(src_dir, dest_dir, save_csv=True, multiple=2):  \n",
    "    \"\"\"\n",
    "    this function takes input path of a dir whoch contains 0 degree images and rotates them to a random angle between \n",
    "    -180 and +180 and stores them in the output dir. also makes a train.csv containing file name and angles.\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    save_dict = {\n",
    "        'filepath': [],\n",
    "        'angle': []\n",
    "    }\n",
    "    \n",
    "\n",
    "    org_img_paths = get_images_in_dir(src_dir, return_path=True)\n",
    "    print(f\"There are {len(org_img_paths)} images in src_folder. Preparing rotated images. \\\n",
    "        \\nmultiple={multiple}. hence there will be {multiple*len(org_img_paths)} images\")\n",
    "    \n",
    "    for num in range(multiple):\n",
    "        for img_path in tqdm(org_img_paths):\n",
    "            img = read_raw_image(img_path, mode='L')\n",
    "            \n",
    "            # select random angle and rotate\n",
    "            angles = np.arange(-180, 180)\n",
    "            angle = random.choice(angles)\n",
    "            img = img.rotate(angle, expand=True)\n",
    "            \n",
    "            # save rotated img in dest folder\n",
    "            img_name, ext = img_path.split('/')[-1].split('.')\n",
    "            save_filename = f\"{img_name}_{angle}.{ext}\"\n",
    "            dest_path = os.path.join(dest_dir, save_filename)       \n",
    "            img.save(dest_path)\n",
    "            \n",
    "            if os.path.exists(dest_path):\n",
    "                save_dict[\"filepath\"].append(dest_path)\n",
    "                save_dict[\"angle\"].append(angle)\n",
    "    \n",
    "    if save_csv==True:\n",
    "        pd.DataFrame(save_dict).to_csv(os.path.join(dest_dir, \"data.csv\"), index=None)\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for folder in ['train', 'test']:\n",
    "\n",
    "#     src_dir = os.path.join(root_dir, f\"data/original/{folder}/\")\n",
    "#     dest_dir = os.path.join(root_dir, f\"data/rotated/{folder}/\")\n",
    "    \n",
    "#     if folder==\"train\": \n",
    "#         multiple=2\n",
    "#     else:\n",
    "#         multiple=1\n",
    "#     prepare_data(src_dir, dest_dir, save_csv=True, multiple=multiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "train_transform=transforms.Compose([\n",
    "    transforms.Resize((400, 400)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.GaussianBlur(3),\n",
    "    # transforms.ColorJitter(0.5),\n",
    "    # transforms.RandomAutocontrast(0.5),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "    # transforms.RandomInvert(0.5),\n",
    "    # transforms.RandomSolarize(0.,0.5)\n",
    "\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((400,400)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class SkewDataset(Dataset):\n",
    "    def __init__(self, csv_path, split=\"test\"):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.filepaths = self.df[\"filepath\"]\n",
    "        self.labels = self.df[\"angle\"]\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = read_raw_image(self.filepaths[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.split==\"train\":\n",
    "            img = train_transform(img)\n",
    "        else:\n",
    "            img = test_transform(img)\n",
    "\n",
    "        return img, torch.tensor(label, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = SkewDataset(\"/home/deepam_minda_farmart_co/fmt/skew_correction/data/rotated/train/data.csv\", split=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "test_dataset = SkewDataset(\"/home/deepam_minda_farmart_co/fmt/skew_correction/data/rotated/test/data.csv\", split=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "# sample = dataset.__getitem__(5)\n",
    "# sample[0].shape, sample[1]\n",
    "# batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_random_images(train_dataset)\n",
    "# tensor2pil(train_dataset.__getitem__(1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:   8%|â–Š         | 2/25 [04:05<46:57, 122.50s/it, loss=1.04e+04, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f56df37b8b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/skew/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1510, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/envs/skew/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1474, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/conda/envs/skew/lib/python3.9/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/conda/envs/skew/lib/python3.9/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/conda/envs/skew/lib/python3.9/multiprocessing/connection.py\", line 936, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/envs/skew/lib/python3.9/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "plot_random_images(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define model class\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(512)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool2d((2,2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(2048, 256)\n",
    "        self.linear2 = nn.Linear(256, 32)\n",
    "        self.linear3 = nn.Linear(32, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.batchnorm2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.batchnorm3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 222, 222]             640\n",
      "       BatchNorm2d-2         [-1, 64, 222, 222]             128\n",
      "              ReLU-3         [-1, 64, 222, 222]               0\n",
      "         MaxPool2d-4         [-1, 64, 111, 111]               0\n",
      "            Conv2d-5        [-1, 256, 109, 109]         147,712\n",
      "       BatchNorm2d-6        [-1, 256, 109, 109]             512\n",
      "              ReLU-7        [-1, 256, 109, 109]               0\n",
      "         MaxPool2d-8          [-1, 256, 54, 54]               0\n",
      "            Conv2d-9          [-1, 512, 52, 52]       1,180,160\n",
      "      BatchNorm2d-10          [-1, 512, 52, 52]           1,024\n",
      "             ReLU-11          [-1, 512, 52, 52]               0\n",
      "        MaxPool2d-12          [-1, 512, 26, 26]               0\n",
      "AdaptiveMaxPool2d-13            [-1, 512, 2, 2]               0\n",
      "          Flatten-14                 [-1, 2048]               0\n",
      "           Linear-15                  [-1, 256]         524,544\n",
      "             ReLU-16                  [-1, 256]               0\n",
      "           Linear-17                   [-1, 32]           8,224\n",
      "             ReLU-18                   [-1, 32]               0\n",
      "           Linear-19                    [-1, 1]              33\n",
      "================================================================\n",
      "Total params: 1,862,977\n",
      "Trainable params: 1,862,977\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 187.88\n",
      "Params size (MB): 7.11\n",
      "Estimated Total Size (MB): 195.18\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "## test the model\n",
    "from torchsummary import summary\n",
    "\n",
    "model = ConvNet()\n",
    "model = model.to(device)\n",
    "summary(model, (1, 224, 224))\n",
    "\n",
    "dummy_input = torch.ones(3,1,224,224, device=device)\n",
    "output = model(dummy_input)\n",
    "# total_params(model)*4/(1024*1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ModelModule(pl.LightningModule):\n",
    "    def __init__(self, model, loss_fn, lr):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # print(f\"input shape {x.size()}, output shape {y.size()}\")\n",
    "        y_hat = self.model(x)\n",
    "        # print(f\"output {y_hat.detach().cpu()}, label {y}\")\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        # acc = get_acc(y_hat, y)\n",
    "        # self.log('train_acc', acc, on_epoch=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        # acc = get_acc(y_hat, y)\n",
    "        # self.log('val_acc', acc, on_epoch=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "        # return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "        return optimizer\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = self.trainer.callback_metrics\n",
    "        print(f'==========> Epoch {self.current_epoch}')\n",
    "        print_metrics_on_epoch_end(metrics, ['train_loss', 'val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy forward pass\n",
    "\n",
    "#  for batch in train_loader:\n",
    "#     x, y = batch\n",
    "#     x, y = x.to(device), y.to(device)\n",
    "#     y_hat = model(x).reshape(-1)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pl_module\n",
    "loss_fn = nn.MSELoss()\n",
    "lr = 1e-4\n",
    "model = ConvNet().to(device)\n",
    "pl_model = ModelModule(model, loss_fn, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n"
     ]
    }
   ],
   "source": [
    "# define trainer\n",
    "\n",
    "verbose=False\n",
    "\n",
    "epochs=100\n",
    "model_string = \"Convnet_custom\"\n",
    "current_date = datetime.now(timezone('Asia/Kolkata')).strftime('%Y-%m-%d')\n",
    "\n",
    "tb_logger = TensorBoardLogger(\n",
    "    save_dir=os.path.join(root_dir, 'logs'), \n",
    "    name=f\"{current_date}-less_aug_&_normalized-{model_string}-{round(total_params(model)/1000000,2)}m_params-{len(train_loader)*train_loader.batch_size}samples-lr{lr}-bs{train_loader.batch_size}\"\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(root_dir, 'checkpoints'),\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    filename=\"{epoch:02d}-{val_loss:.2f}-{val_acc:.2f}-{model_string}\"\n",
    ")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0.00, patience=5, verbose=True, mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        max_epochs=epochs, \n",
    "        logger=tb_logger, \n",
    "        log_every_n_steps=1, \n",
    "        limit_train_batches=1, \n",
    "        limit_val_batches=None,\n",
    "        enable_checkpointing=False,\n",
    "        # callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/deepam_minda_farmart_co/fmt/skew_correction/logs/2023-08-19-less_aug_&_normalized-Convnet_custom-1.86m_params-1536samples-lr0.0001-bs32\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | model   | ConvNet | 1.9 M \n",
      "1 | loss_fn | MSELoss | 0     \n",
      "------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "7.452     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.63it/s]==========> Epoch 0\n",
      "{'val_loss': 10817.09}\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:07<00:00,  3.22it/s, loss=1.04e+04, v_num=0]==========> Epoch 0\n",
      "{\n",
      "    \"train_loss\": 10396.99,\n",
      "    \"val_loss\": 11131.89\n",
      "}\n",
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.65it/s, loss=1.04e+04, v_num=0]==========> Epoch 1\n",
      "{\n",
      "    \"train_loss\": 10381.66,\n",
      "    \"val_loss\": 11131.74\n",
      "}\n",
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:22<00:00,  1.11it/s, loss=1.04e+04, v_num=0]==========> Epoch 2\n",
      "{\n",
      "    \"train_loss\": 10371.86,\n",
      "    \"val_loss\": 11131.59\n",
      "}\n",
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:29<00:00,  1.20s/it, loss=1.04e+04, v_num=0]==========> Epoch 3\n",
      "{\n",
      "    \"train_loss\": 10363.25,\n",
      "    \"val_loss\": 11131.41\n",
      "}\n",
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:37<00:00,  1.49s/it, loss=1.04e+04, v_num=0]==========> Epoch 4\n",
      "{\n",
      "    \"train_loss\": 10360.29,\n",
      "    \"val_loss\": 11131.21\n",
      "}\n",
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:44<00:00,  1.79s/it, loss=1.04e+04, v_num=0]==========> Epoch 5\n",
      "{\n",
      "    \"train_loss\": 10358.14,\n",
      "    \"val_loss\": 11131.0\n",
      "}\n",
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:53<00:00,  2.12s/it, loss=1.04e+04, v_num=0]==========> Epoch 6\n",
      "{\n",
      "    \"train_loss\": 10359.82,\n",
      "    \"val_loss\": 11130.81\n",
      "}\n",
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:00<00:00,  2.42s/it, loss=1.04e+04, v_num=0]==========> Epoch 7\n",
      "{\n",
      "    \"train_loss\": 10359.54,\n",
      "    \"val_loss\": 11130.64\n",
      "}\n",
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:08<00:00,  2.72s/it, loss=1.04e+04, v_num=0]==========> Epoch 8\n",
      "{\n",
      "    \"train_loss\": 10361.53,\n",
      "    \"val_loss\": 11130.48\n",
      "}\n",
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:15<00:00,  3.02s/it, loss=1.04e+04, v_num=0]==========> Epoch 9\n",
      "{\n",
      "    \"train_loss\": 10357.27,\n",
      "    \"val_loss\": 11130.31\n",
      "}\n",
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:22<00:00,  3.32s/it, loss=1.04e+04, v_num=0]==========> Epoch 10\n",
      "{\n",
      "    \"train_loss\": 10356.91,\n",
      "    \"val_loss\": 11130.11\n",
      "}\n",
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:30<00:00,  3.61s/it, loss=1.04e+04, v_num=0]==========> Epoch 11\n",
      "{\n",
      "    \"train_loss\": 10356.62,\n",
      "    \"val_loss\": 11129.85\n",
      "}\n",
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:37<00:00,  3.91s/it, loss=1.04e+04, v_num=0]==========> Epoch 12\n",
      "{\n",
      "    \"train_loss\": 10356.67,\n",
      "    \"val_loss\": 11129.53\n",
      "}\n",
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [01:45<00:00,  4.23s/it, loss=1.04e+04, v_num=0]==========> Epoch 13\n",
      "{\n",
      "    \"train_loss\": 10357.32,\n",
      "    \"val_loss\": 11129.12\n",
      "}\n",
      "Epoch 14:   8%|â–Š         | 2/25 [01:49<20:54, 54.56s/it, loss=1.04e+04, v_num=0] "
     ]
    }
   ],
   "source": [
    "trainer.fit(pl_model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more things to try - \n",
    "- [ ] normalize image\n",
    "- [ ] batchnorm\n",
    "- [ ] more augmentations\n",
    "- [ ] cleaning data\n",
    "- ~~different lr (e-4, e-5 )~~\n",
    "\n",
    "!!shuffle off for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
