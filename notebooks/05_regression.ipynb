{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/deepam_minda_farmart_co/fmt/skew_correction'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from skew_correction.helper import *\n",
    "\n",
    "root_dir = \"/\".join( os.getcwd().split(\"/\")[:-1])\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_a_dir_into_train_test(src_dir, dest_dir):\n",
    "#     org_img_paths = get_images_in_dir(src_dir, return_path=True)\n",
    "    \n",
    "\n",
    "\n",
    "def split_files(src_dir, dest_dir, train_size):\n",
    "    if not os.path.exists(dest_dir):\n",
    "        os.makedirs(dest_dir)\n",
    "    \n",
    "    all_files = os.listdir(src_dir)\n",
    "    random.shuffle(all_files)\n",
    "    \n",
    "    total_files = len(all_files)\n",
    "    train_count = int(total_files * train_size)\n",
    "    \n",
    "    train_files = all_files[:train_count]\n",
    "    test_files = all_files[train_count:]\n",
    "    \n",
    "    train_dir = os.path.join(dest_dir, \"train_\")\n",
    "    test_dir = os.path.join(dest_dir, \"test_\")\n",
    "    \n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    \n",
    "    move_files(src_dir, train_dir, train_files)\n",
    "    move_files(src_dir, test_dir, test_files)\n",
    "    return train_files, test_files\n",
    "\n",
    "def move_files(src_dir, dest_dir, files):\n",
    "    for file in files:\n",
    "        src_path = os.path.join(src_dir, file)\n",
    "        dest_path = os.path.join(dest_dir, file)\n",
    "        shutil.move(src_path, dest_path)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "src_directory = os.path.join(data_dir, \"original/train\")\n",
    "dest_directory = os.path.join(data_dir, \"original/\")\n",
    "train_ratio = 0.8  # 80% for training, 20% for testing\n",
    "\n",
    "## train_files, test_files = split_files(src_directory, dest_directory, train_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(src_dir, dest_dir, save_csv=True, multiple=2):  \n",
    "    \"\"\"\n",
    "    this function takes input path of a dir whoch contains 0 degree images and rotates them to a random angle between \n",
    "    -180 and +180 and stores them in the output dir. also makes a train.csv containing file name and angles.\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    save_dict = {\n",
    "        'filepath': [],\n",
    "        'angle': []\n",
    "    }\n",
    "    \n",
    "\n",
    "    org_img_paths = get_images_in_dir(src_dir, return_path=True)\n",
    "    print(f\"There are {len(org_img_paths)} images in src_folder. Preparing rotated images. \\\n",
    "        \\nmultiple={multiple}. hence there will be {multiple*len(org_img_paths)} images\")\n",
    "    \n",
    "    for num in range(multiple):\n",
    "        for img_path in tqdm(org_img_paths):\n",
    "            img = read_raw_image(img_path, mode='L')\n",
    "            \n",
    "            # select random angle and rotate\n",
    "            angles = np.arange(-180, 180)\n",
    "            angle = random.choice(angles)\n",
    "            img = img.rotate(angle, expand=True)\n",
    "            \n",
    "            # save rotated img in dest folder\n",
    "            img_name, ext = img_path.split('/')[-1].split('.')\n",
    "            save_filename = f\"{img_name}_{angle}.{ext}\"\n",
    "            dest_path = os.path.join(dest_dir, save_filename)       \n",
    "            img.save(dest_path)\n",
    "            \n",
    "            if os.path.exists(dest_path):\n",
    "                save_dict[\"filepath\"].append(dest_path)\n",
    "                save_dict[\"angle\"].append(angle)\n",
    "    \n",
    "    if save_csv==True:\n",
    "        pd.DataFrame(save_dict).to_csv(os.path.join(dest_dir, \"data.csv\"), index=None)\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 955 images in src_folder. Preparing rotated images.         \n",
      "multiple=2. hence there will be 1910 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/955 [00:00<02:52,  5.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 955/955 [01:19<00:00, 12.04it/s]\n",
      "100%|██████████| 955/955 [01:18<00:00, 12.23it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_dir = os.path.join(root_dir, \"data/original/train/\")\n",
    "dest_dir = os.path.join(root_dir, \"data/rotated/train/\")\n",
    "prepare_data(src_dir, dest_dir, save_csv=True, multiple=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "train_transform=transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class SkewDataset(Dataset):\n",
    "    def __init__(self, csv_path, split=\"test\"):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.filepaths = self.df[\"filepath\"]\n",
    "        self.labels = self.df[\"angle\"]\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = read_raw_image(self.filepaths[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.split==\"train\":\n",
    "            img = train_transform(img)\n",
    "        else:\n",
    "            img = transform(img)\n",
    "\n",
    "        return img, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SkewDataset(\"/home/deepam_minda_farmart_co/fmt/skew_correction/data/rotated/train/data.csv\")\n",
    "# sample = dataset.__getitem__(5)\n",
    "# sample[0].shape, sample[1]\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "# ?DataLoader\n",
    "# batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define model class\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool2d((2,2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.linear1 = nn.Linear(1024, 256)\n",
    "        self.linear2 = nn.Linear(256, 32)\n",
    "        self.linear3 = nn.Linear(32, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 222, 222]             640\n",
      "              ReLU-2         [-1, 64, 222, 222]               0\n",
      "         MaxPool2d-3         [-1, 64, 111, 111]               0\n",
      "            Conv2d-4        [-1, 256, 109, 109]         147,712\n",
      "              ReLU-5        [-1, 256, 109, 109]               0\n",
      "         MaxPool2d-6          [-1, 256, 54, 54]               0\n",
      "            Conv2d-7          [-1, 256, 52, 52]         590,080\n",
      "              ReLU-8          [-1, 256, 52, 52]               0\n",
      "         MaxPool2d-9          [-1, 256, 26, 26]               0\n",
      "AdaptiveMaxPool2d-10            [-1, 256, 2, 2]               0\n",
      "          Flatten-11                 [-1, 1024]               0\n",
      "           Linear-12                  [-1, 256]         262,400\n",
      "             ReLU-13                  [-1, 256]               0\n",
      "           Linear-14                   [-1, 32]           8,224\n",
      "             ReLU-15                   [-1, 32]               0\n",
      "           Linear-16                    [-1, 1]              33\n",
      "================================================================\n",
      "Total params: 1,009,089\n",
      "Trainable params: 1,009,089\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 118.15\n",
      "Params size (MB): 3.85\n",
      "Estimated Total Size (MB): 122.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = ConvNet(5)\n",
    "summary(model, (1, 224, 224))\n",
    "\n",
    "dummy_input = torch.ones(3,1,224,224)\n",
    "output = model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "78,785*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x128 and 512x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[184], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m linear1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(\u001b[39m512\u001b[39m, \u001b[39m128\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m linear1(output\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n",
      "File \u001b[0;32m/opt/conda/envs/skew/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/skew/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x128 and 512x128)"
     ]
    }
   ],
   "source": [
    "linear1 = nn.Linear(512, 128)\n",
    "linear1(output.unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# flat(output.unsqueeze(0))\n",
    "output.unsqueeze(0).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?nn.AdaptiveMaxPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
